---
title: Smart Universal Sustainability System Yielding Biodegradability Analysis of
  Known Agents
author: "Drusany J, Ploj Y"
date: "2023-01-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Exploration

```{r}
data <- read.csv("train.csv")
df <- data
testing_data <- read.csv("test.csv")
head(data)
```

We prepared some functions to help us with the analysis.
```{r}
mcf <- function(data) {
    #' Return the major class frequency per each column
    return(sapply(data, function(x) max(table(x)) / sum(table(x))))
}
correlation <- function(data, threshold) {
    #' Return pairs of features with correlations above a threshold
    mat = cor(na.omit(data[, -ncol(data)]))
    mat[lower.tri(mat, diag = TRUE)] <- 0
    correlations <- which(abs(mat) > threshold, arr.ind = TRUE)
    rownames(correlations) <- NULL
    colnames(correlations) <- NULL
    annotated = t(apply(
        correlations,
        1,
        function(x) c(x[1], x[2], mat[x[1], x[2]])
    ))
    return(annotated)
}
```

### General data analysis
Percentage of rows with missing features:
```{r}
print(sum(apply(data, 1, function(x) any(is.na(x)))) / nrow(data))
```
Number of missing features per row:
```{r}
print(table(apply(data, 1, function(x) sum(is.na(x)))))
```
Total missing features by column:
```{r}
missing_values <- apply(data, 2, function(x) sum(is.na(x)))
print(missing_values[missing_values > 0])
```
Class counts:
```{r}
groups = split(data, data$Class)
print(sapply(groups, nrow))
```
Number of unique values per column:
```{r}
print(sapply(data, function(x) length(unique(x))))
```
Major class frequency per column:
```{r}
MCF = mcf(data)
print(MCF)
print(MCF[MCF > 0.95])
```
Correlation between features:
```{r}
print(correlation(data, 0.90))
```
Variance of features:
```{r}
print(apply(data, 2, var))
```
Outliers per column:
```{r}
for (col in 1:ncol(data)) {
    s = colnames(data)[col]
    if (typeof(data[,col]) != "double")
        num = 0
    else 
        num = sum(
            data[,col] < quantile(data[,col], 0.05, na.rm=TRUE) |
            data[,col] > quantile(data[,col], 0.95, na.rm=TRUE)
        ) / length(data[,col])
    print(paste(s, num, sep=": "))
}
```

### Visualizations
```{r}
colour <- function(datatype) ifelse(datatype == "double", "blue", "red")

save_plots <- function(data, width, height, plot_function, plots_per_page = c(1,1), plot_count=NaN, filename=function(page) page) {
    #' Generate N plots
    if (is.na(plot_count)) plot_count = ncol(data)
	  if (typeof(filename) == "character") filename = local(function(page) filename, list2env(list(filename=filename)))
    for (page in 1:floor(plot_count / prod(plots_per_page))) {
		    name = as.character(filename(page))
        png(paste(name, ".png", sep=""), width, height)
        par(mfrow=plots_per_page)
        start = (page-1) * prod(plots_per_page) + 1
        end = min(plot_count, page * prod(plots_per_page))
        for (col in start:end) {
            plot_function(col)
        }
        dev.off()
    }
}
save_boxplots <- function(data, width=1000, height=1500, filename="boxplot") {
    #' Generate boxplots of each column
    save_plots(
        data=data,
        width=width,
        height=height,
        plot_function=function(col)
            boxplot(data[,colnames(data)[col]], width=1, border=colour(typeof(data[,col]))),
        plots_per_page=c(4,11),
		filename=filename
    )
}
save_barplots <- function(data, width=1000, height=2000, filename="barplot") {
    #' Generate barplots of each column
    save_plots(
        data=data,
        width=width,
        height=height,
        plot_function=function(col)
            barplot(table(data[,col]), main=colnames(data)[col], border=colour(typeof(data[,col]))),
        plots_per_page=c(11,4),
		filename=filename
    )
}
save_scatterplots <- function(data, width=10000, height=10000, filename="scatterplot") {
    #' Generate scatterplots of each column, colour them by class
    save_plots(
        data=data,
        width=width,
        height=height,
        plot_function=function(col)
            pairs(data, col=adjustcolor(c("red", "blue", alpha=0.5)[data$Class])),
        plots_per_page=c(1,1),
        plot_count=1,
		filename=filename
    )
}

#save_boxplots(df, filename="images/boxes")
#save_boxplots(outliers.winsorize(df), filename="images/boxes_winsorized")
#save_barplots(outliers.winsorize(df), filename="images/histograms")
#save_scatterplots(df, filename="images/scatters")
#save_scatterplots(transform.pca(impute.mean(df)), filename="images/scatters_pca")
```

![Boxplots](images/boxes.png)
![Boxplots Winsorized](images/boxes_winsorized.png)
![Histograms](images/histograms.png)
![Scatterplots](images/scatters.png)
![Scatterplots PCA](images/scatters_pca.png)

## Preprocessing transformers

In light of the findings above, we designed the following transformers:

```{r}
dist <- function(datum, data) {
    #' Calculate the distance from datum to each row in data
    return(apply(data, 1, function(x) sqrt(sum((as.integer(datum) - as.integer(x))^2))))
}

transform.pca <- function(data) {
    #' Transform data using PCA
    pca <- prcomp(data[,-ncol(data)], center=TRUE, scale=TRUE)
    ret = data.frame(pca$x)
    ret$Class <- data$Class
    return(ret)
}

outliers.winsorize <- function(data, threshold=0.05) {
    #' Winsorize outliers
    for (i in 1:ncol(data)) {
        if (typeof(data[,i]) != "double") next
        quantiles <- quantile(
            data[,i],
            probs=c(threshold, 1-threshold),
            na.rm=TRUE
        )
        data[,i] <- ifelse(data[,i] < quantiles[1],
            quantiles[1],
            ifelse(data[,i] > quantiles[2],
                quantiles[2],
                data[,i]
            )
        )
    }
    return(data)
}

impute.mean <- function(data) {
    #' Impute missing values with the mean of the column
    for (i in 1:ncol(data)) {
        if (typeof(data[,i]) %in% c("double", "integer")) {
            data[,i] <- ifelse(is.na(data[,i]),
                mean(data[,i], na.rm=TRUE),
                data[,i]
            )
        }
    }
    return(data.frame(data))
}
impute.filter <- function(data) {
    #' Remove rows with missing data
    return(data[complete.cases(data),])
}
impute.knn <- function(data, k=1) {
	#' Impute using k nearest neighbours
    to_impute = data[!complete.cases(data),]
    for (row in rownames(to_impute)) {
        incomplete_row = colnames(data[row,])[apply(data[row,], 2, anyNA)]
        incomplete_filter = (names(data[row,]) %in% incomplete_row) # booleans
        distances = dist(data[row,!incomplete_filter], data[complete.cases(data),!incomplete_filter])
        k_nearest = data[complete.cases(data),][order(distances)[1:k],]
        
        # replace only na's
        for (col in incomplete_row) {
            if (typeof(data[,col]) %in% c("double", "integer")) {
                data[row,col] <- ifelse(is.na(data[row,col]),
                    mean(k_nearest[,col], na.rm=TRUE),
                    data[row,col]
                )
            }
        }
    }
    return(data.frame(data))
}

prune.mcf <- function(data, cutoff = 0.95) {
    #' Prune features with major class frequency > cutoff
    MCF = mcf(data)
    return(data[, names(MCF[MCF <= cutoff])])
}
prune.variance <- function(data, cutoff = 0.1) {
    #' Prune features with variance < cutoff
    return(data[, apply(data, 2, function(x) var(x, na.rm=TRUE)) >= cutoff])
}
prune.correlated <- function(data, threshold) {
    #' Prune features that are highly correlated with each other
    correlations = correlation(data, threshold)
    return(data[, !colnames(data) %in% correlations[,1]])
}
```


## Classifiers

```{r}
library(MASS)
library(naivebayes)
library(randomForest)

classify.majority.train <- function(data) {
    #' Train a majority class classifier
    return(names(which.max(table(data$Class))))
}
classify.majority.execute <- function(classifier, datum) {
    #' Classify using majority class
    return(classifier)
}

classify.random.train <- function(data) {
    #' Train a random classifier
    return(c())
}
classify.random.execute <- function(classifier, datum) {
    #' Classify using random class
    return(sample(2, 1))
}

classify.lda.train <- function(data) {
    #' Train a linear discriminant analysis classifier
    return(lda(Class ~ ., data=data))
}
classify.lda.execute <- function(classifier, datum) {
    #' Classify using linear discriminant analysis
    return(predict(classifier, datum)$class)
}

classify.bayes.train <- function(data) {
    #' Train a naive bayes classifier
    # # first convert all integer columns to factors
    # for (i in 1:ncol(data)) {
    #     if (typeof(data[,i]) == "integer") {
    #         data[,i] = as.factor(data[,i])
    #     }
    # }
    return(naive_bayes(Class ~ ., data=data, laplace=1))
}
classify.bayes.execute <- function(classifier, datum) {
    #' Classify using naive bayes
    return(predict(classifier, datum))
}

classify.random_forest.train <- function(data) {
    #' Train a random forest classifier
    return(randomForest(Class ~ ., data=data))
}
classify.random_forest.execute <- function(classifier, datum) {
    #' Classify using random forest
    return(predict(classifier, datum))
}


# Combine classify.X.train and classify.X.execute into classify.X
# classify.X(training_data) returns a function that can be used to classify.
classifiers = c("majority", "random", "bayes", "lda", "random_forest")
for (classifier in classifiers) {
	train = match.fun(paste("classify.", classifier, ".train", sep=""))
	execute = match.fun(paste("classify.", classifier, ".execute", sep=""))

	name = paste("classify.", classifier, sep="")

	fun = local(function(training_data) {
		classifier = train(training_data)
        columns = names(training_data)
        columns = columns[columns != "Class"]
		    return(function(testing_datum) {
          results = list()
          for (i in rownames(testing_datum)) {
              results[[i]] <- execute(classifier, testing_datum[i,columns])
          }
          return(results)
        })
	}, list2env(list(train=train, execute=execute)))
	assign(name, fun)
}
```

## Cross-validation and testing

```{r}
library(pryr)

df$Class = as.factor(df$Class)
testing_data$Class = as.factor(testing_data$Class)
preprocess = compose(outliers.winsorize, prune.mcf, prune.variance, impute.filter)
```
```{r}
k_fold_validate <- function(data, k, classifier) {
    #' Perform k-fold cross validation on a dataset
    fold_size = round(nrow(data) / k)
    fold_indices = seq(1, nrow(data), fold_size)
    fold_indices = c(fold_indices, nrow(data)+1)
    fold_accuracies = rep(1, k)
    for (fold in 1:k) {
        test_indices = seq(fold_indices[fold], fold_indices[fold+1]-1)
        fold_data = data[-test_indices,]
        fold_test = data[test_indices,]
        fold_classifier = classifier(fold_data)
        classified = fold_classifier(fold_test)
        correct = as.integer(classified) == fold_test$Class
        fold_accuracies[fold] = sum(correct) / nrow(fold_test)
    }
    return(fold_accuracies)
}
k_fold = k_fold_validate(preprocess(df), 5, classify.random_forest)
k_fold
mean(k_fold)
```

```{r}
classifier = classify.random_forest(preprocess(df))
results = c()
for (row in rownames(testing_data)) {
    test = testing_data[row, names(testing_data) != "Class"]
    results = c(results, tryCatch(as.integer(classifier(test)), error=function(e) "error"))
}
contingency_table = table(results, testing_data$Class)
print(contingency_table)
print(sum(diag(contingency_table))/sum(contingency_table))
```
